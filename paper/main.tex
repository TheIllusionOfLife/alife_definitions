\documentclass[letterpaper]{article}
\usepackage{natbib,alifeconf}
\usepackage{graphicx,amsmath,amssymb,booktabs,xcolor,multirow}
\usepackage{url,hyperref}
\usepackage{ifthen}
\newcommand{\includefigure}[2][width=\columnwidth]{%
  \IfFileExists{#2}{\includegraphics[#1]{#2}}{%
    \fbox{\parbox{0.8\columnwidth}{\centering\vspace{1em}%
      \textit{Figure placeholder}\vspace{1em}}}}}

% --- Result values (from analysis pipeline; calibration: seeds 0--99, held-out test: seeds 100--199, 5 regimes) ---
\newcommand{\kappaDD}[1]{%
  \ifthenelse{\equal{#1}{D1D2}}{0.89}{%
  \ifthenelse{\equal{#1}{D1D3}}{0.48}{%
  \ifthenelse{\equal{#1}{D1D4}}{0.31}{%
  \ifthenelse{\equal{#1}{D2D3}}{0.42}{%
  \ifthenelse{\equal{#1}{D2D4}}{0.25}{%
  \ifthenelse{\equal{#1}{D3D4}}{0.16}{??}}}}}}}
\newcommand{\rhoDD}[1]{%
  \ifthenelse{\equal{#1}{D1D2}}{0.68}{%
  \ifthenelse{\equal{#1}{D1D3}}{0.78}{%
  \ifthenelse{\equal{#1}{D1D4}}{0.75}{%
  \ifthenelse{\equal{#1}{D2D3}}{0.59}{%
  \ifthenelse{\equal{#1}{D2D4}}{0.42}{%
  \ifthenelse{\equal{#1}{D3D4}}{0.62}{??}}}}}}}
\newcommand{\fleissK}{0.43}
\newcommand{\aucD}[1]{%
  \ifthenelse{\equal{#1}{D1}}{0.84}{%
  \ifthenelse{\equal{#1}{D2}}{0.65}{%
  \ifthenelse{\equal{#1}{D3}}{0.76}{%
  \ifthenelse{\equal{#1}{D4}}{0.78}{??}}}}}
\newcommand{\TODO}[1]{\textbf{[#1]}}
% --- End result values ---

\title{Life Definitions Disagree: An Empirical Benchmark\\of Competing Operationalizations in a Shared Digital Ecology}
\author{Anonymous}

\begin{document}
\maketitle

% ============================================================================
\begin{abstract}
Multiple operational definitions of life exist, yet they are rarely applied to
the same system under controlled conditions.  We present an empirical benchmark
comparing four definitions---textbook 7-criteria (D1), Darwinian/NASA (D2),
autonomy/organizational closure (D3), and information maintenance (D4)---by
scoring the same organism populations across five environment regimes in a
shared digital ecology.  Three co-existing family types with different ablated
capabilities serve as natural controls.  We report the disagreement structure
among definitions, identify regime-dependent divergences, and evaluate
predictive validity against population robustness.  Our benchmark suite and
adapter code are released as an open artifact for the ALife community.
\end{abstract}

Submission type: \textbf{Full Paper}

% ============================================================================
\section{Introduction}
\label{sec:intro}

What counts as ``alive'' depends on whom you ask.  Textbook biology
lists seven properties (metabolism, growth, reproduction, etc.); NASA's
working definition foregrounds Darwinian evolution; autopoiesis theory
emphasizes organizational closure; and information-theoretic accounts
stress causal self-maintenance of heritable information
\citep{campbell_2017_biology, joyce_1994_foreword, varela_1974_autopoiesis,
schrodinger_1944_what}.  Each definition captures a genuine aspect of living
systems, yet they are rarely applied to the same system under controlled
conditions.  A researcher who adopts one definition may classify a digital
organism as ``alive'' while another, using a different definition, rejects it.

This disagreement is not merely philosophical.  In astrobiology, the choice
of life definition determines which biosignatures to search for
\citep{benner_2010_defining}.  In artificial life, it shapes which systems
we consider successful.  Yet no prior work has systematically compared
multiple operational definitions on the same digital organisms in the same
environment.

We address this gap by constructing an \emph{empirical benchmark}.  We
implement four definitions as scoring adapters that consume identical
organism trace data from a shared digital ecology.  Three family types---one
with all capabilities enabled (F1), one lacking boundary and homeostasis
(F2, ``Darwinian''), and one lacking reproduction and evolution
(F3, ``autonomy'')---coexist in five environment regimes, providing natural
controls where definitions are expected to disagree.  We report:
\begin{enumerate}
\item The pairwise agreement structure (Cohen's $\kappa$, Spearman $\rho$)
  among D1--D4.
\item Regime-dependent divergences where environmental stress alters
  which definitions accept or reject a family.
\item Predictive validity: which definition's graded score best predicts
  robust population persistence under unseen conditions.
\end{enumerate}

All code, adapters, and benchmark data are released as an open artifact.

% ============================================================================
\section{Related Work}
\label{sec:related}

\paragraph{Definitions of life.}
\citet{campbell_2017_biology} codifies the textbook 7-criteria list.
The NASA/Darwinian definition, attributed to \citet{joyce_1994_foreword},
frames life as ``a self-sustaining chemical system capable of Darwinian
evolution.''  \citet{varela_1974_autopoiesis} introduced autopoiesis---the
idea that living systems are defined by organizational closure,
where constituent processes mutually produce the network that produces them.
\citet{schrodinger_1944_what} anticipated the information-theoretic view,
emphasizing that organisms maintain internal order against entropy;
\citet{walker_2013_algorithmic} extend this to an algorithmic framework
where life is distinguished by top-down informational causation.

\citet{bedau_2008_nature} surveys the landscape of life definitions and
argues that no single criterion is sufficient.
\citet{ruizmirazo_2004_universal} propose a synthesis of \emph{basic
autonomy} (metabolic self-maintenance) and \emph{open-ended evolution},
bridging the D2--D3 tension that our benchmark foregrounds.
\citet{cleland_2002_defining} and \citet{benner_2010_defining} further
argue that each definition captures a necessary but insufficient
aspect.  Our benchmark operationalizes this pluralist insight: rather than
choosing one definition, we instrument all four and compare.

\paragraph{ALife benchmarking.}
Digital evolution platforms such as Avida \citep{ofria_2004_avida} and
cellular-automaton worlds like Lenia \citep{chan_2019_lenia} and
Flow-Lenia \citep{plantec_2023_flow} have been used to study individual
properties of life (e.g., open-ended evolution, self-organization).
\citet{dolson_2019_modes} introduced the MODES toolbox for measuring
open-ended dynamics, providing statistical diagnostics for evolutionary
systems; however, these efforts typically adopt a single operational
criterion and do not cross-compare definitions on the same organisms.
Our contribution is the \emph{benchmarking methodology}: a shared substrate,
controlled family ablations, and a pluggable adapter API that allows new
definitions to be added without modifying the simulator.

% ============================================================================
\section{System and Baseline}
\label{sec:system}

\paragraph{Hybrid substrate.}
Our digital ecology uses a two-layer agent-based model implemented in Rust
with Python bindings (PyO3/maturin).  The lower layer consists of
\emph{swarm agents} (10--50 per organism) that execute a small neural
network controlling movement, energy harvesting, waste excretion, and
boundary maintenance.  The upper layer aggregates agents into
\emph{organisms}: coherent spatial clusters that can grow, reproduce
(by fission), and die.  Organisms inhabit a continuous 2D toroidal
world with diffusing resource and waste fields.
The world $\to$ organism $\to$ agent hierarchy means that each organism's
behavior emerges from collective agent dynamics, while population-level
patterns emerge from organism interactions.

Each organism carries a heritable genome (256 floats) encoding neural-network
weights.  Reproduction introduces point mutations, enabling Darwinian
dynamics.  A configurable set of Boolean flags controls which capabilities
are active per family (metabolism, boundary maintenance, homeostasis,
stimulus response, reproduction, evolution, growth).

\paragraph{Simulation modes.}
The simulator supports two modes.  \emph{Mode~A} runs a single homogeneous
population (no family structure); it is useful for baseline dynamics but
provides no internal controls.  \emph{Mode~B} configures multiple
co-existing family types, each with distinct capability flags, sharing the
same world.  We use Mode~B for the main benchmark because the co-existing
family ablations serve as natural controls where definitions are expected to
disagree, enabling within-run comparisons.

\paragraph{Mode~B configuration.}
All benchmark runs place three family types in the same
world, competing for the same resources.  Each family starts with 10
organisms (30 total), each with 25 agents.  Runs proceed for 2{,}000
time steps with samples every 10 steps (200 sample points), providing
sufficient time-series length for transfer entropy and Granger causality
estimation.

To disentangle competition effects from capability deficits, we additionally
run \emph{single-family controls}: Mode~B with a single family entry,
preserving the full adapter pipeline while eliminating inter-family
resource competition (see \S\ref{sec:experiments}).

\paragraph{Family profiles.}
\textbf{F1} (full) has all seven capabilities enabled and serves as the
positive control.  \textbf{F2} (Darwinian) disables boundary maintenance
and homeostasis, retaining reproduction and evolution---it should satisfy
D2 but score lower on D3.  \textbf{F3} (autonomy) disables reproduction
and evolution, retaining metabolism, boundary maintenance, homeostasis,
and growth---it should satisfy D3 but fail D2.

% ============================================================================
\section{Definition Adapters}
\label{sec:adapters}

Each adapter consumes a run summary (JSON) and a family ID, and returns a
graded score $S \in [0, 1]$ plus a binary pass/fail label at a configurable
threshold (default 0.3).  All adapters share a common \texttt{AdapterResult}
dataclass.

\subsection{D1: Textbook 7-Criteria}

D1 operationalizes the textbook list
\citep{campbell_2017_biology} by assessing each of seven criteria
through three conditions.  Table~\ref{tab:d1_mapping} maps each criterion
to its simulator variable and downstream coupling target.

\begin{table}[t]
\centering
\small
\caption{D1 criterion-to-variable mapping.}
\label{tab:d1_mapping}
\begin{tabular}{@{}lll@{}}
\toprule
Criterion & Primary signal & Coupling target \\
\midrule
Metabolism     & \texttt{energy\_mean}   & \texttt{waste\_mean} \\
Organization   & \texttt{boundary\_mean} & \texttt{energy\_mean} \\
Homeostasis    & \texttt{energy\_mean}   & \texttt{alive\_count} \\
Growth         & \texttt{maturity\_mean} & \texttt{alive\_count} \\
Reproduction   & \texttt{birth\_count}   & \texttt{alive\_count} \\
Stimuli resp.  & \texttt{boundary\_mean} & \texttt{energy\_mean} \\
Evolution      & \texttt{genome\_diversity} & \texttt{alive\_count} \\
\bottomrule
\end{tabular}
\end{table}

The three conditions are:
\begin{itemize}
\item $\alpha$ --- \emph{Dynamic process}: the coefficient of variation (CV)
  of the criterion's primary signal exceeds a noise floor, confirming
  active rather than static behavior.
\item $\beta$ --- \emph{Measurable degradation}: Cohen's $d$ effect size
  comparing the target family's signal against a family that \emph{lacks}
  the criterion (cross-family ablation test).
\item $\gamma$ --- \emph{Feedback coupling}: the maximum of transfer entropy
  \citep{schreiber_2000_measuring} and lagged cross-correlation (max lag~5)
  from the criterion's signal to its downstream target.
  The cross-correlation complement provides robustness when transfer entropy
  estimates are noisy at moderate sample sizes.
\end{itemize}

Each criterion score is $w_\alpha \cdot \alpha + w_\beta \cdot
\sigma(\beta) + w_\gamma \cdot \gamma$ with default weights
$(w_\alpha, w_\beta, w_\gamma) = (0.3, 0.4, 0.3)$.
The aggregate is the geometric mean of all seven criteria.
The geometric mean ensures that \emph{any} criterion
scoring zero pulls the aggregate to zero, making D1 the strictest
definition.  We verify robustness to weight choice via $\pm 20\%$
perturbation analysis (\S\ref{sec:results}).

\subsection{D2: Darwinian / NASA}

D2 operationalizes ``a self-sustaining chemical system capable of Darwinian
evolution'' \citep{joyce_1994_foreword} through three sub-scores:

\begin{itemize}
\item $S_\text{reprod}$ --- Sustained reproduction: persistence of per-step
  births in the last 50\% of the run, combined with lineage event count.
\item $S_\text{hered}$ --- Heritability: intergenerational genome hash
  stability, approximating $h^2$ from default mutation parameters.
\item $S_\text{select}$ --- Differential success via the Price equation
  \citep{price_1970_selection}: $\Delta\bar{z} = \text{Cov}(w,z)/\bar{w}$,
  where $w$ is per-parent offspring count (fitness) and $z$ is mean
  parent--child genome distance (L2 norm).  This decomposes selection
  directly from lineage records, replacing the earlier correlation-based
  proxy.
\end{itemize}

The aggregate is the geometric mean of all three, ensuring that a family
with zero reproduction (F3) scores zero regardless of heritability or
selection signals.

\subsection{D3: Autonomy / Organizational Closure}

D3 operationalizes autopoiesis \citep{varela_1974_autopoiesis} as a
graph property.  Five process variables (energy, waste, boundary,
birth count, maturity) form nodes.  Directed edges are detected by
combining transfer entropy and Granger causality (max lag~5) in a
two-level correction scheme: (1)~per-pair Bonferroni for the two tests
($p_\text{pair} = \min(1,\; 2 \cdot \min(p_\text{TE}, p_\text{Granger}))$),
then (2)~Benjamini--Hochberg FDR control at $q = 0.05$
\citep{benjamini_1995_controlling} across all $5 \times 4 = 20$ pairs.

The \emph{closure score} is the fraction of process variables in the
largest strongly connected component (SCC) of the influence graph
(Tarjan's algorithm; singleton SCCs excluded).
To avoid circularity with the alive-count AUC prediction target,
the primary D3 score uses closure alone (\texttt{closure\_only} mode).
A secondary composite, closure $\times$ persistence, is reported
separately; it measures self-maintenance duration (fraction of steps
where population exceeds 50\% of initial size) but is not used in
predictive validity evaluation.
A non-reproducing family that persists through self-maintenance (F3)
can score well on D3 under either mode.
Sensitivity to the FDR $q$-value is assessed at $q \in \{0.01, 0.05, 0.10\}$
(\S\ref{sec:results}).

\subsection{D4: Information Maintenance}

D4 frames life as a system where heritable information causally maintains
itself \citep{schrodinger_1944_what, walker_2013_algorithmic}.
Three sub-scores:

\begin{itemize}
\item $S_\text{present}$ --- Non-trivial heritable information exists:
  genome drift $> 0$ and diversity compared to cross-family baseline.
\item $S_\text{causal}$ --- Information predicts fitness: Spearman
  $\rho$ between genome diversity and alive count, plus transfer
  entropy from diversity to alive count (bonus for significance).
\item $S_\text{preserved}$ --- Information persists across generations:
  Jaccard similarity of genome hashes between consecutive generations.
  Genome hashes use FNV-1a over the little-endian byte representation
  of the 256-float genome (offset \texttt{0xcbf29ce484222325},
  prime \texttt{0x100000001b3}).
\end{itemize}

The aggregate is a weighted average with double weight on the causal
component: $(S_\text{present} + 2 \cdot S_\text{causal} +
S_\text{preserved}) / 4$.

% ============================================================================
\section{Experiments}
\label{sec:experiments}

\paragraph{Environment regimes.}
Five regimes stress-test different aspects of organism viability.
We state a priori predictions for each:
\textbf{E1} (baseline)---all families viable, D1--D4 agree on F1;
\textbf{E2} (resource scarcity: halved regeneration, larger world)---metabolic
stress expected to reduce D3 closure for all families;
\textbf{E3} (crowding: 80 organisms, 30 agents, smaller world)---competition
pressure expected to amplify D2/D3 divergence;
\textbf{E4} (sensing noise: Gaussian $\sigma=0.5$ on neural inputs)---should
degrade D1's $\gamma$ condition (coupling via TE) more than D2's
reproductive statistics;
\textbf{E5} (spatial patchiness: 4 resource patches, $2\times$ peak
regeneration)---spatial heterogeneity may advantage organisms with
better stimulus response.

\paragraph{Data separation.}
Following pre-registration best practice, we split seeds into
\emph{calibration} (seeds 0--99) and \emph{held-out test} (seeds 100--199).
All threshold tuning and adapter development use only calibration seeds.
Test seeds are evaluated with frozen thresholds; ``unseen conditions''
refers to unseen seeds within the same regimes, not unseen regimes.

\paragraph{Single-family controls.}
To disentangle competition from capability deficits, each family type is
also run alone (Mode~B with a single family entry) across all five
regimes and 200 seeds.  This preserves the full scoring pipeline
(\texttt{family\_breakdown} output is identical) while eliminating
inter-family resource competition.

\paragraph{Scoring.}
Each (regime $\times$ seed $\times$ family) triple is scored by all four
adapters, producing a score matrix of $5 \times 100 \times 3 \times 4 =
6{,}000$ graded scores (calibration set).
The default pass/fail threshold is 0.3, chosen as a round value below the
empirical median score of the positive control (F1) to separate viable
from non-viable families; calibrated per-definition thresholds are reported
in Table~\ref{tab:thresholds}.

\begin{table}[t]
\centering
\small
\caption{Calibrated thresholds (maximizing balanced accuracy on seeds 0--99)
vs.\ the default 0.3.}
\label{tab:thresholds}
\begin{tabular}{@{}lcc@{}}
\toprule
Definition & Default & Calibrated \\
\midrule
D1 & 0.30 & \emph{TBD} \\
D2 & 0.30 & \emph{TBD} \\
D3 & 0.30 & \emph{TBD} \\
D4 & 0.30 & \emph{TBD} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Agreement metrics.}
Pairwise agreement: Cohen's $\kappa$ (binary), Spearman $\rho$ (graded),
percent agreement.  Aggregate: Fleiss' $\kappa$ for 4-rater agreement.
Block bootstrap CIs at the seed level preserve within-seed correlation.

\paragraph{Predictive validity.}
For each definition, we sweep thresholds on the calibration set to
maximize balanced accuracy of predicting whether the alive-count AUC
over the last 30\% of the run exceeds the calibration-set median.
Optimal thresholds are frozen, then evaluated on test seeds via
ROC-AUC, precision, recall, and a $\pm 20\%$ sensitivity analysis.
To test robustness against substrate bias, we additionally evaluate
two alternative prediction targets: \emph{recovery time} (normalized
steps to recover population after the deepest dip) and
\emph{lineage diversity} (ratio of unique reproducing parent genotypes
to total birth events in the final 20\% of the lineage).

\paragraph{Sensitivity analysis.}
Three sweeps assess robustness of adapter parameterization:
(1)~D1 weight perturbation ($\pm 20\%$ on $w_\alpha, w_\beta, w_\gamma$),
(2)~binary threshold sweep (0.1--0.9 for all definitions), and
(3)~D3 FDR $q$-value sweep ($q \in \{0.01, 0.05, 0.10\}$).

\paragraph{Compute cost.}
Each 2{,}000-step run takes ${\sim}$25--30\,s on a Mac Mini M2 Pro.
The main benchmark (5 regimes $\times$ 200 seeds) requires ${\sim}$8\,h;
single-family controls (3 families $\times$ 5 regimes $\times$ 200 seeds)
add ${\sim}$22\,h.  Analysis scripts (Python) run in ${\sim}$2\,h.

% ============================================================================
\section{Results}
\label{sec:results}

\paragraph{Disagreement heatmap.}
Figure~\ref{fig:heatmap} shows mean D1--D4 scores across regime--family
combinations.  The expected diagonal pattern emerges: F1 scores highly
on all definitions, F2 scores highly on D2 but lower on D3, and F3
scores near zero on D2 but retains moderate D3 scores.  D1 is
uniformly strict for ablated families due to its geometric-mean
aggregate.

\begin{figure}[t]
\centering
\includefigure{figures/disagreement_heatmap.pdf}
\caption{Disagreement heatmap.  Each cell shows the mean definition score
across seeds.  Rows: regime $\times$ family; columns: D1--D4.}
\label{fig:heatmap}
\end{figure}

\paragraph{Agreement matrix.}
Figure~\ref{fig:agreement} presents the $4 \times 4$ agreement matrix.
D1 and D2 show the highest pairwise \emph{binary} agreement
($\kappa = $ \kappaDD{D1D2}), reflecting their shared reliance on
reproduction and evolutionary signals, while \emph{graded} agreement
is highest for D1--D3 ($\rho = $ \rhoDD{D1D3}).
D3 and D4 show the lowest $\kappa$ (\kappaDD{D3D4}),
as organizational closure and information maintenance capture
fundamentally different aspects.  The D1--D3 pair shows moderate
agreement ($\kappa = $ \kappaDD{D1D3}), while D2--D4 is weaker
($\kappa = $ \kappaDD{D2D4}).  The aggregate Fleiss' $\kappa$ across
all four definitions is \fleissK{} (95\% bootstrap CI: [0.40, 0.46]).

\begin{figure}[t]
\centering
\includefigure[width=0.85\columnwidth]{figures/agreement_matrix.pdf}
\caption{Agreement matrix.  Lower triangle: Cohen's $\kappa$ (binary);
upper triangle: Spearman $\rho$ (graded).}
\label{fig:agreement}
\end{figure}

\paragraph{Case studies.}
Figure~\ref{fig:casestudy} illustrates two diagnostic disagreements.
\emph{Left panel}: F3 (autonomy) maintains stable energy and boundary
signals throughout the run, earning a moderate D3 closure score,
while D2 assigns zero due to absent reproduction.
\emph{Right panel}: F2 (Darwinian) shows sustained births and genome
diversity, earning high D2 scores, but its broken boundary--energy
feedback loop reduces D3 closure.

\begin{figure}[t]
\centering
\includefigure{figures/case_study_timeseries.pdf}
\caption{Case study time series.  Left: F3 (D3 $>$ D2).
Right: F2 (D2 $>$ D3).}
\label{fig:casestudy}
\end{figure}

\paragraph{D3 closure-only vs.\ composite.}
Separating closure from persistence resolves the circularity concern.
Under \texttt{closure\_only} mode, D3 scores reflect purely the
organizational closure of the influence graph.  The D2--D3 disagreement
axis is preserved: F3 achieves non-zero closure (mean largest SCC includes
1.6 of 5 process variables) while scoring zero on D2.
The composite (closure $\times$ persistence) shows similar patterns
but with attenuated scores for non-reproducing families.

\paragraph{Price equation selection.}
The Price decomposition yields positive selection coefficients for F1
(mean $S_\text{select} = 0.30$) and F2 (mean $S_\text{select} = 0.48$),
confirming that differential reproductive success correlates with genome
distance.  F3, lacking reproduction, returns zero.  The Price-based
$S_\text{select}$ sub-score strengthens D2's discrimination: F1 and F2
show clear selection signals absent in F3.

\paragraph{Competition controls.}
Single-family runs (3 families $\times$ 5 regimes $\times$ 200 seeds)
confirm that the F1 $>$ F2 $>$ F3 ordering on D1 holds in isolation
across all regimes (E1: 0.88 $>$ 0.80 $>$ 0.47).
F3's D3 closure score is \emph{higher} without competition
(0.37 vs.\ 0.21 in E1, $\Delta = +0.16$), confirming that resource
deprivation from F1 partially suppresses F3's self-maintenance in
coexistence.  F2's D1 score also rises substantially in isolation
(0.80 vs.\ 0.34 in E1), showing that competition amplifies apparent
capability differences.  Crucially, the D2--D3 disagreement axis is
preserved: F3 still receives D2 = 0 but D3 $>$ 0 in both conditions.

\paragraph{Predictive validity.}
Figure~\ref{fig:roc} shows ROC performance on the held-out test set
(Table~\ref{tab:predictive}).
D1 achieves the highest predictive validity (ROC-AUC = \aucD{D1}),
followed by D4 (\aucD{D4}) and D3 (\aucD{D3}).
D2 shows the lowest AUC (\aucD{D2}), likely because its geometric-mean
aggregation produces a narrow score range that limits threshold discrimination.
D1's composite multi-criteria approach captures the broadest signal.

\begin{table}[t]
\centering
\small
\caption{Predictive validity on test seeds (100--199).
Primary target: alive-count AUC.}
\label{tab:predictive}
\begin{tabular}{@{}lcccc@{}}
\toprule
 & D1 & D2 & D3 & D4 \\
\midrule
ROC-AUC          & \aucD{D1} & \aucD{D2} & \aucD{D3} & \aucD{D4} \\
Balanced Acc.\    & 0.84 & 0.66 & 0.74 & 0.78 \\
Precision         & 0.99 & 0.63 & 0.72 & 1.00 \\
Recall            & 0.70 & 0.78 & 0.80 & 0.55 \\
\bottomrule
\end{tabular}
\end{table}

Under the alternative \emph{recovery time} target, D1 retains the
highest AUC (0.92), while D4 rises to second (0.87), ahead of D3 (0.79)
and D2 (0.67).  Under the \emph{lineage diversity} target (parent
reproductive diversity), all definitions achieve comparable AUC:
D1 (0.83), D4 (0.78), D2 (0.78), and D3 (0.77).
This compressed ranking confirms that the definitions capture
complementary information: the alive-count hierarchy does not
generalize to all ecological outcomes.

\begin{figure}[t]
\centering
\includefigure[width=0.85\columnwidth]{figures/predictive_roc.pdf}
\caption{ROC curves on held-out test seeds (100--199).  Points mark
the frozen threshold from calibration.}
\label{fig:roc}
\end{figure}

\paragraph{Sensitivity analysis.}
D1 weight perturbation ($\pm 20\%$) changes mean scores by at most
0.019 but preserves the F1 $>$ F2 $>$ F3 ordering across all
tested perturbation vectors.
The binary threshold sweep shows monotonically decreasing pass rates
as thresholds increase from 0.1 to 0.9 for all definitions.
D3 FDR sweep: relaxing $q$ from 0.01 to 0.10 increases mean significant
edges from 6.9 to 8.7 and mean closure from 0.48 to 0.61, but the
qualitative agreement structure is preserved.

\paragraph{Surrogate false positive rate.}
Phase-randomized surrogate analysis (100 surrogates per variable pair,
50 seeds, E1 regime) yields an overall mean FPR of 0.078, below the
0.10 target; no individual coupling pair's mean FPR exceeds 0.11,
within acceptable margin of the nominal threshold.
Only the source series is randomized, preserving the real target's
autocorrelation structure to test directional TE specifically.
This confirms that at $n = 200$ sample points with 5 bins, the TE-based
coupling detection is not systematically producing false positives.

% ============================================================================
\section{Discussion}
\label{sec:discussion}

\paragraph{What the disagreements reveal.}
The D2--D3 disagreement axis is the most informative: it cleanly separates
evolution-centric from organization-centric perspectives.  F3 satisfies D3
but not D2; F2 satisfies D2 but scores lower on D3.  This is not a flaw
in either definition but a genuine ontological divergence about which
processes are essential---echoing the ``basic autonomy + open-ended
evolution'' synthesis of \citet{ruizmirazo_2004_universal}.
D1, by requiring \emph{all} seven criteria, subsumes both perspectives
but is too strict to distinguish borderline cases.

\paragraph{Regime dependence.}
Under E2 (resource scarcity), even F1 shows reduced D3 closure because
stress disrupts feedback coupling.  Under E4 (sensing noise), D1 scores
drop more sharply than D2 scores, because noisy sensory inputs degrade
the transfer-entropy coupling ($\gamma$) condition while leaving
reproductive statistics intact.  This suggests that definition-level
sensitivity to environment perturbation is itself an informative
diagnostic.

\paragraph{Competition vs.\ capability.}
Single-family controls confirm that the qualitative disagreement structure
persists when inter-family competition is removed.  F3's D3 closure score
improves modestly in isolation, indicating that resource competition with
F1 partially suppresses self-maintenance in coexistence runs.  However,
the D2--D3 divergence axis is preserved, confirming that definition
disagreements reflect capability differences, not competition artifacts.

\paragraph{D1 cross-family dependence.}
The $\beta$ condition of D1 compares a family against one that lacks the
criterion being tested.  This means D1 scores depend on the composition
of co-existing families; adding a new family type could shift scores.
We view this as inherent to comparative definitions: the ``measurable
degradation'' test requires a reference.  In single-family runs where
no ablation partner exists, $\beta$ defaults to zero, and D1 relies on
$\alpha$ and $\gamma$ alone.

\paragraph{Predictive validity trade-offs.}
D1 achieves the highest predictive validity for population robustness
(ROC-AUC = \aucD{D1}), followed by D4 (\aucD{D4}), D3 (\aucD{D3}),
and D2 (\aucD{D2}).  D1's composite multi-criteria approach captures
the broadest signal, while D2's geometric-mean aggregation narrows its
score range.  Alternative targets partially mitigate substrate bias:
under lineage diversity all four definitions converge to a narrow
AUC band (0.77--0.83), confirming complementary information capture.
We caution against interpreting
predictive validity as a ranking of definitions' ``correctness''---it is
substrate- and target-dependent.

\paragraph{Limitations.}
(1)~\emph{Adapter dependence}: each definition's score depends on its
operationalization.  Different operationalizations of the same definition
could shift agreement patterns.  Sensitivity analysis shows that
$\pm 20\%$ weight perturbation preserves family ordering, but larger
changes to the operationalization (e.g., replacing geometric mean with
arithmetic mean in D1) remain unexplored.
(2)~\emph{Substrate bias}: the digital ecology's design choices (e.g.,
explicit reproduction mechanism, continuous-valued 256-float genomes)
may favor evolution-centric definitions.  Alternative prediction targets
partially address this, but testing on fundamentally different substrates
(e.g., chemical reaction networks, Lenia-like CAs) is needed.
(3)~\emph{Price equation transmission}: we report only the selection
component of the Price equation; the transmission component (requiring
multi-generation lineage tracking) is left for future work.

\paragraph{Extensibility.}
The adapter API is designed for community extension.  Adding a fifth
definition (e.g., agency-based, thermodynamic) requires implementing a
single function with signature \texttt{score(run\_summary, family\_id)
$\to$ AdapterResult}.  The benchmark harness handles data loading,
scoring, and analysis automatically.

% ============================================================================
\section{Artifact Release}
\label{sec:artifact}

Code, adapters, and raw benchmark data are available at an anonymous
repository (link provided upon acceptance; code under MIT license)
and Zenodo (data, CC-BY~4.0; DOI to be assigned upon acceptance).

\paragraph{Reproduction.}
\begin{verbatim}
git clone <repo> && cd alife_definitions
uv sync && uv run maturin develop
bash scripts/reproduce_all.sh
\end{verbatim}

The single script runs the full pipeline: benchmark generation
(${\sim}$30\,h for coexistence + single-family controls on a Mac Mini
M2 Pro), scoring, agreement analysis, predictive validity, sensitivity
analysis, and figure generation.

\paragraph{Data availability.}
Raw benchmark JSONs (${\sim}$10\,GB for 5 regimes $\times$ 200 seeds
$\times$ coexistence + single-family at 200 sample points each)
are archived on Zenodo.  The score matrix TSV and analysis JSONs are
included in the repository.

% ============================================================================
\section*{Acknowledgments}
Removed for double-blind review.

\bibliographystyle{apalike}
\bibliography{references}

\end{document}
